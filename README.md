## Overview
This project is designed to scrape data from the Tuya S.A. website, process the data using a language model, and generate answers to specific questions. The project utilizes web scraping, data storage, and a language model pipeline to achieve its goals.

## Structure
The project is organized into several sections:
1. **Libraries**: Importing necessary libraries.
2. **Environment Variables**: Loading environment variables.
3. **Questions**: Defining the questions to be answered.
4. **Web Scraping**: Functions to scrape data from the website.
5. **Saving Scraping Models**: Functions to save the scraped data.
6. **LLM (Language Model)**: Setting up the language model and tokenizer.
7. **Upload Data**: Functions to upload the scraped data.
8. **Querying the Model**: Functions to query the language model with specific questions.
9. **Saving Answers**: Functions to save the generated answers.


## Usage
1. **Web Scraping**: Run the notebook cells to scrape data from the Tuya S.A. website.
2. **Save Data**: Save the scraped data into JSON files.
3. **Upload Data**: Load the saved data into the notebook.
4. **Query the Model**: Use the language model to answer specific questions based on the scraped data.
5. **Save Answers**: Save the generated answers into text files.

## Files
- `app.py`: This is the orquestrator file, which it can carry out all executions.
- `Data_Saver.py`: This class organize and save data from scrapeing and responses obtained by LLama Model.
- `LlamaQuerier.py`: This class performs and return answers following a Llama Model.
- `scraping.py`: Contains functions to save the scraped data and generated answers.
- `Notebook_Test.ipynb`: Jupyter notebook containing the entire workflow.

## Example Questions
1. **Question 1**: ¿Cuáles son los nombres de las tarjetas que tiene disponibles Tuya S.A.?
2. **Question 2**: ¿Cuáles son los valores de la tasa de interés y póliza del producto Credicompras?

## Results
The answers to the questions are generated by the language model and saved into text files.